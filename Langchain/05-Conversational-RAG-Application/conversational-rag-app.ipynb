{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7f32f3e",
   "metadata": {},
   "source": [
    "### **Conversational RAG** Application with Langchain & OpenAI LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96250be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74b6836",
   "metadata": {},
   "source": [
    "#### Initialize OpenAI LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a137bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# set openAI API key\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Initialize that ChatOpenAI model\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01952a1b",
   "metadata": {},
   "source": [
    "#### Initializig Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "beff8e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa3000a",
   "metadata": {},
   "source": [
    "#### Load PDF Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0bf61a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Load the PDF document\n",
    "loader = PyPDFLoader(\".\\storytelling with data.pdf\")\n",
    "doc = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "551c9d03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96c2aac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'moddate': '2025-03-14T14:53:05+05:30', 'source': '.\\\\storytelling with data.pdf', 'total_pages': 75, 'page': 0, 'page_label': '1'}, page_content='Storytelling \\nwitǝ DatƝ\\nVisual Analytics and User Experience Design\\n(IT4031)')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edca18f5",
   "metadata": {},
   "source": [
    "#### Split document into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82a7bf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Initialize the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=250, chunk_overlap=20)\n",
    "\n",
    "# Split the document into chunks\n",
    "splits = text_splitter.split_documents(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e905e7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2ce6d2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The most powerful person in \\nthe world is the storyteller.\\nThe storyteller sets the \\nvision, values, and agenda of \\nan entire generation that is to \\ncome.\\n- Steve Jobs\\n“\\n“'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits[20].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ccdc81",
   "metadata": {},
   "source": [
    "#### Create Vector Store and Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ecca35a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "# create a vector store from the document chunks\n",
    "vector_store = Chroma.from_documents(documents=splits, embedding=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0ea43d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever from the vector store\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692a1b0e",
   "metadata": {},
   "source": [
    "#### Define Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5eca1c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define the system prompt\n",
    "system_prompt = (\n",
    "    \"You are an intelligent chat bot. Use the following context to answer the question. If you don't know the answer, just say you don't know.\" \n",
    "    \"\\n\\n\" \n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "# create the prompt template\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "958e2e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"You are an intelligent chat bot. Use the following context to answer the question. If you don't know the answer, just say you don't know.\\n\\n{context}\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927746c2",
   "metadata": {},
   "source": [
    "#### Create Retrieval Augmented Generation (RAG) Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e6d1877",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Function to join documents\n",
    "def join_docs(docs):\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "# Final RAG chain (CORRECT)\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | join_docs,\n",
    "        \"input\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4e8fa6",
   "metadata": {},
   "source": [
    "#### Invoke RAG chain with example question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b49911f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_chain.invoke(\"What is the meaning of storytelling with data?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af6b364b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_chain.invoke(\"what are things we should learn under storytelling with data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bfff23cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_chain.invoke(\"can you list down?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c089659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, I don't have access to specific lists or information about how bad things can get. If you have a specific question or topic in mind, feel free to ask!\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541cb301",
   "metadata": {},
   "source": [
    "### Add Chat History"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6620cd4c",
   "metadata": {},
   "source": [
    "#### Create History Aware Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "16e6f425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.prompts import MessagesPlaceholder\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# # Define contextualize system prompt\n",
    "# contextualize_system_prompt = (\n",
    "#     \"Given a chat history and the latest user question, \"\n",
    "#     \"reformulate the question if it refers to context in the chat history. \"\n",
    "#     \"Otherwise, return the question as is. \"\n",
    "#     \"ONLY return the reformulated question, nothing else.\"\n",
    "# )\n",
    "\n",
    "# contextualize_prompt = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         (\"system\", contextualize_system_prompt),\n",
    "#         MessagesPlaceholder(\"chat_history\"),\n",
    "#         (\"human\", \"{input}\")\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# # Create a chain to convert chat history + question into a standalone question\n",
    "# standalone_question_chain = contextualize_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6dba37",
   "metadata": {},
   "source": [
    "#### Create History Aware RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "66bf2059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the main system prompt\n",
    "# system_prompt = (\n",
    "#     \"You are an intelligent chatbot. Use the following context to answer the question. \"\n",
    "#     \"If you don't know the answer based on the context, just say you don't know. \"\n",
    "#     \"Use the chat history to understand the context of the conversation.\\n\\n\"\n",
    "#     \"Context:\\n{context}\"\n",
    "# )\n",
    "\n",
    "# # Create the main prompt template\n",
    "# prompt = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         (\"system\", system_prompt),\n",
    "#         MessagesPlaceholder(\"chat_history\"),\n",
    "#         (\"human\", \"{input}\")\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45edf2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "# def format_docs(docs):\n",
    "#     return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# # Create the complete RAG chain\n",
    "# rag_chain = (\n",
    "#     {\n",
    "#         # Get chat history\n",
    "#         \"chat_history\": lambda x: x.get(\"chat_history\", []),\n",
    "#         # Create standalone question from history and input\n",
    "#         \"standalone_question\": lambda x: standalone_question_chain.invoke({\n",
    "#             \"input\": x[\"input\"], \n",
    "#             \"chat_history\": x.get(\"chat_history\", [])\n",
    "#         }) if x.get(\"chat_history\") else x[\"input\"],\n",
    "#         # Keep original input\n",
    "#         \"input\": lambda x: x[\"input\"]\n",
    "#     }\n",
    "#     | RunnableLambda(lambda x: {\n",
    "#         # Retrieve documents using standalone question\n",
    "#         \"context\": retriever.get_relevant_documents(x[\"standalone_question\"]),\n",
    "#         \"chat_history\": x[\"chat_history\"],\n",
    "#         \"input\": x[\"input\"]\n",
    "#     })\n",
    "#     | RunnableLambda(lambda x: {\n",
    "#         # Format the documents\n",
    "#         \"context\": format_docs(x[\"context\"]),\n",
    "#         \"chat_history\": x[\"chat_history\"],\n",
    "#         \"input\": x[\"input\"]\n",
    "#     })\n",
    "#     | prompt\n",
    "#     | llm\n",
    "#     | StrOutputParser()\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d09dca1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b88be7ca",
   "metadata": {},
   "source": [
    "#### Manage Chat Session History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "db8e581b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.chat_history import BaseChatMessageHistory\n",
    "# from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "# from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# # Initialize the store for session history\n",
    "# store = {}\n",
    "\n",
    "# # Function to get the session history for a given session ID\n",
    "# def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "#     if session_id not in store:\n",
    "#         store[session_id] = ChatMessageHistory()\n",
    "#     return store[session_id]\n",
    "\n",
    "# conversational_rag_chain = RunnableWithMessageHistory(\n",
    "#     rag_chain,\n",
    "#     get_session_history,\n",
    "#     input_messages_key=\"input\",\n",
    "#     history_messages_key=\"chat_history\",\n",
    "#     output_messages_key=\"answer\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7d699ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test the chain\n",
    "# response = conversational_rag_chain.invoke(\n",
    "#     {\"input\": \"What is the meaning of storytelling with data?\"},\n",
    "#     config={\"configurable\": {\"session_id\": \"101\"}}\n",
    "# )\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e285973",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8ef51249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storytelling with data refers to the practice of presenting data in a meaningful and engaging manner by weaving it into a narrative. It involves using data visualization techniques to effectively communicate insights, trends, and key findings to an audience in a way that is easy to understand and remember. This approach helps make data more impactful and actionable for decision-making purposes.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Define contextualize system prompt\n",
    "contextualize_system_prompt = (\n",
    "    \"Given a chat history and the latest user question, \"\n",
    "    \"reformulate the question if it refers to context in the chat history. \"\n",
    "    \"Otherwise, return the question as is. \"\n",
    "    \"ONLY return the reformulated question, nothing else.\"\n",
    ")\n",
    "\n",
    "contextualize_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a chain to convert chat history + question into a standalone question\n",
    "standalone_question_chain = contextualize_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Function to format documents\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Define the main system prompt\n",
    "system_prompt = (\n",
    "    \"You are an intelligent chatbot. Use the following context to answer the question. \"\n",
    "    \"If you don't know the answer based on the context, just say you don't know. \"\n",
    "    \"Use the chat history to understand the context of the conversation.\\n\\n\"\n",
    "    \"Context:\\n{context}\"\n",
    ")\n",
    "\n",
    "# Create the main prompt template\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the complete RAG chain\n",
    "rag_chain = (\n",
    "    {\n",
    "        # Get chat history\n",
    "        \"chat_history\": lambda x: x.get(\"chat_history\", []),\n",
    "        # Create standalone question from history and input\n",
    "        \"standalone_question\": lambda x: standalone_question_chain.invoke({\n",
    "            \"input\": x[\"input\"], \n",
    "            \"chat_history\": x.get(\"chat_history\", [])\n",
    "        }) if x.get(\"chat_history\") else x[\"input\"],\n",
    "        # Keep original input\n",
    "        \"input\": lambda x: x[\"input\"]\n",
    "    }\n",
    "    | RunnableLambda(lambda x: {\n",
    "        # Retrieve documents using standalone question\n",
    "        \"context\": retriever.invoke(x[\"standalone_question\"]),  # Use invoke() instead\n",
    "        \"chat_history\": x[\"chat_history\"],\n",
    "        \"input\": x[\"input\"]\n",
    "    })\n",
    "    | RunnableLambda(lambda x: {\n",
    "        # Format the documents\n",
    "        \"context\": format_docs(x[\"context\"]),\n",
    "        \"chat_history\": x[\"chat_history\"],\n",
    "        \"input\": x[\"input\"]\n",
    "    })\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Initialize the store for session history\n",
    "store = {}\n",
    "\n",
    "# Function to get the session history for a given session ID\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# Create the conversation RAG with session history\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"output\"\n",
    ")\n",
    "\n",
    "# Test the chain\n",
    "response = conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What is the meaning of storytelling with data?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"101\"}}\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0d63733c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under storytelling with data, there are several key things to learn and master:\n",
      "\n",
      "1. Data Visualization: Understanding how to create effective charts, graphs, and other visual representations of data to convey information clearly and efficiently.\n",
      "\n",
      "2. Narrative Building: Learning how to structure a compelling story around the data, including setting the context, presenting the main points, and drawing conclusions.\n",
      "\n",
      "3. Audience Understanding: Knowing your audience and tailoring your data storytelling approach to their needs, preferences, and level of understanding.\n",
      "\n",
      "4. Data Interpretation: Developing the ability to analyze data accurately and draw meaningful insights that can be communicated effectively.\n",
      "\n",
      "5. Communication Skills: Enhancing your communication skills to present data in a clear, concise, and engaging manner, whether through written reports, presentations, or data visualizations.\n",
      "\n",
      "By mastering these aspects of storytelling with data, you can effectively convey the significance of your data and drive informed decision-making within your organization.\n"
     ]
    }
   ],
   "source": [
    "response = conversational_rag_chain.invoke(\n",
    "    {\"input\": \"what are things we should learn under storytelling with data\"},\n",
    "    config={\"configurable\": {\"session_id\": \"101\"}}\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a85b67e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data storytelling is a powerful technique that involves presenting data in a compelling narrative to convey insights and drive decision-making. By mastering data visualization, narrative building, audience understanding, data interpretation, and communication skills, you can effectively engage your audience and make data more impactful and actionable. Through clear and engaging storytelling with data, you can bring your insights to life and inspire informed decisions within your organization.\n"
     ]
    }
   ],
   "source": [
    "response = conversational_rag_chain.invoke(\n",
    "    {\"input\": \"can you give me a short paragraph\"},\n",
    "    config={\"configurable\": {\"session_id\": \"101\"}}\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc9f0f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (rag_env)",
   "language": "python",
   "name": "rag_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
